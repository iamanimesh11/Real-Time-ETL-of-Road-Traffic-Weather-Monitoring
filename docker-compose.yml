services:
  loki:
    image: grafana/loki:latest
    container_name: loki
    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    
    ports:
      - "3100:3100"
    volumes:
      - ./config/loki-config.yml:/etc/loki/local-config.yml
      - ./loki_data:/loki
      - ./shared:/shared
    
    command: -config.file=/etc/loki/local-config.yml

  promtail:
    image: grafana/promtail:latest
    container_name: promtail
    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    volumes:
      - ./config/promtail-config.yml:/etc/promtail/promtail-config.yml
      - ./pipelines/airflow/logs:/opt/airflow/logs  # âœ… Ensure Promtail can read Airflow logs
      - ./shared:/shared
    command: -config.file=/etc/promtail/promtail-config.yml 
    depends_on:
      - loki  # âœ… Ensures Promtail starts after Loki is running


  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=animesh16  # Set your custom password here

    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    ports:
      - "3000:3000"
    volumes:
      - ./grafana_data:/var/lib/grafana
      - ./shared:/shared
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards


    user: "0"  # ðŸ‘ˆ run as root
    depends_on:
      - loki


  postgres:
    image: postgres:latest
    container_name: postgres
    restart: always
    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: animesh11
      POSTGRES_DB: de_personal
    healthcheck:  # âœ… Ensure PostgreSQL is ready before Airflow starts
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 5s
      retries: 5
      timeout: 3s
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./shared:/shared
      - ./config/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql



  airflow-init:
    image: apache/airflow:latest  # âœ… No need for custom build
    container_name: airflow-init
    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    depends_on:
      - postgres
    
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:animesh11@postgres:5432/de_personal
      - AIRFLOW__CORE__LOAD_EXAMPLES=False  
    command: airflow db init
    volumes: 
      - ./shared:/shared

  

  airflow-webserver:
      build:
        context: ./pipelines/airflow
        dockerfile: Dockerfile
      image: airflow_webserver_image  # âœ… No need for custom build 
      container_name: airflow-webserver
      networks:
        - my_custom_network
      dns:
        - 8.8.8.8
        - 8.8.4.4
      ports:
        - "8080:8080"
      restart: always

      depends_on:
        airflow-init:
          condition: service_completed_successfully
        postgres:
          condition: service_healthy

      healthcheck:
        test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
        interval: 30s
        retries: 5
        timeout: 10s

      environment:
        - TZ=Asia/Kolkata
        - AIRFLOW__WEBSERVER__AUTH_BACKEND=airflow.api.auth.backend.default
        - AIRFLOW_USER=${AIRFLOW_USER}
        - AIRFLOW_PASSWORD=${AIRFLOW_PASSWORD}
        - AIRFLOW__CORE__EXECUTOR=LocalExecutor
        - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:animesh11@postgres:5432/de_personal
        - AIRFLOW__WEBSERVER__SECRET_KEY=mysecretkey
        - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags

      command: >
          bash -c "
          airflow db upgrade &&
          airflow users create --username $AIRFLOW_USER --password $AIRFLOW_PASSWORD --firstname Animesh --lastname User --role Admin --email animesh@example.com || true &&
          airflow webserver"

      volumes:
        - ./pipelines/airflow/airflow.cfg:/opt/airflow/airflow.cfg
        - ./pipelines/airflow/dags:/opt/airflow/dags
        - ./pipelines/airflow/logs:/opt/airflow/logs
        - ./pipelines/scripts:/opt/airflow/scripts
        - ./pipelines/airflow/plugins:/opt/airflow/plugins
        - ./common:/opt/airflow/common       
        - ./config:/opt/airflow/config
        - ./shared:/shared


  airflow-scheduler:
    build:
        context: ./pipelines/airflow
        dockerfile: Dockerfile
    image: airflow_scheduler_image  # âœ… No need for custom build
    container_name: airflow-scheduler
    restart: always
    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    depends_on:
      airflow-webserver:
        condition: service_healthy
    environment:
      - TZ=Asia/Kolkata
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:animesh11@postgres:5432/de_personal
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
    command: airflow scheduler
    env_file:
        - ./.env
    volumes:
      - ./pipelines/airflow/airflow.cfg:/opt/airflow/airflow.cfg
      - ./pipelines/airflow/dags:/opt/airflow/dags
      - ./pipelines/airflow/logs:/opt/airflow/logs
      - ./pipelines/scripts:/opt/airflow/scripts
      - ./pipelines/kafka_files:/opt/airflow/kafka_files
      - ./pipelines/airflow/plugins:/opt/airflow/plugins
      - ./common:/opt/airflow/common       
      - ./config:/opt/airflow/config
      - ./shared:/shared



  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    restart: always
    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper
      - ./shared:/shared
    healthcheck:
          test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep imok"]
          interval: 5s
          timeout: 3s
          retries: 3
          start_period: 10s
      

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    restart: always
    depends_on:
      - zookeeper
    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    ports:
       - "9092:9092"
    environment:
        KAFKA_BROKER_ID: 1
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT

        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
        - kafka_data:/var/lib/kafka
        - ./shared:/shared
 


  kafka-consumer:
    build: 
        context: ./pipelines/kafka_files
        dockerfile: Dockerfile_consumer_traffic
    container_name: kafka_road_consumer
    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    depends_on:
      - kafka  # Ensure Kafka is running before the consumer starts
      - postgres  # Ensure DB is up
    environment:
      - KAFKA_BROKER=kafka:9092
      - POSTGRES_DB=de_personal
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=animesh11

    volumes:
      - ./pipelines/kafka_files:/app/pipelines/kafka_files
      - ./pipelines/scripts:/opt/airflow/scripts
      - ./common:/opt/airflow/common 
      - ./shared:/shared
    entrypoint: ["/bin/sh", "-c", "/shared/wait-for-flag.sh && python /app/pipelines/kafka_files/road_consumer.py"]

    
    
  kafka-consumer-traffic:
    build: 
      context: ./pipelines/kafka_files
      dockerfile: ./Dockerfile_consumer_traffic
    container_name: kafka-consumer-traffic
    depends_on:
      - kafka  # Ensure Kafka is running before the consumer starts
      - postgres  # Ensure DB is up
    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    environment:
      - KAFKA_BROKER=kafka:9092
      - POSTGRES_DB=de_personal
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=animesh11

    
    volumes:
      - ./pipelines/kafka_files:/app/pipelines/kafka_files
      - ./pipelines/scripts:/opt/airflow/scripts
      - ./common:/opt/airflow/common 
      - ./shared:/shared
    entrypoint: ["/bin/sh", "-c", "chmod +x /shared/wait-for-flag.sh && /shared/wait-for-flag.sh && python /app/pipelines/kafka_files/traffic_consumer.py"]


  


  
  
  
  
  streamlit:
    build: ./common/streamlit
    container_name: streamlit
    networks:
      - my_custom_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
    ports:
      - "8501:8501"
      - "2375:2375"  # Expose Docker port

    depends_on:
      - kafka
    environment:
      - KAFKA_BROKER=kafka:9092
    
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  # Add this line to mount the Docker socket
      - ./common/streamlit:/app  # Mounts the local "streamlit" folder into the container
      - ./shared:/shared  

    command: ["streamlit", "run", "main_Streamlit.py", "--server.port=8501", "--server.address=0.0.0.0", "--server.runOnSave=true"]


 
networks:
  my_custom_network:
    driver: bridge  # Or other network driver as needed
    
    
volumes:
  loki_data:
  grafana_data:
  postgres_data:  
  airflow-logs:
  airflow-dags:
  zookeeper_data:
  kafka_data: